{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks Insight Agent - Example Analysis\n",
    "\n",
    "This notebook demonstrates how to use the Databricks Insight Agent for enterprise security and network analysis.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Databricks Insight Agent is a multi-agent system that analyzes Databricks system tables to provide actionable insights on:\n",
    "- Network activity patterns\n",
    "- Security threats and anomalies\n",
    "- Operational performance\n",
    "- Compliance monitoring\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Databricks workspace with system table access\n",
    "- OpenAI API key\n",
    "- Required Python packages installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "# %pip install databricks-sdk dspy-ai mlflow pandas openai structlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "# Add project path\n",
    "project_path = Path(\"../src\")\n",
    "if str(project_path) not in sys.path:\n",
    "    sys.path.append(str(project_path))\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup configuration\n",
    "from utils.config import config\n",
    "from utils.logging import setup_logging\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(log_level=\"INFO\")\n",
    "\n",
    "# Configure credentials (replace with your values)\n",
    "config.set('DATABRICKS_HOST', 'https://your-workspace.cloud.databricks.com')\n",
    "config.set('DATABRICKS_TOKEN', 'your-databricks-token')\n",
    "config.set('OPENAI_API_KEY', 'your-openai-api-key')\n",
    "\n",
    "print(\"Configuration setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agents\n",
    "from agents.orchestrator import OrchestratorAgent\n",
    "from agents.reporting import ReportingAgent\n",
    "\n",
    "# Create orchestrator\n",
    "orchestrator = OrchestratorAgent()\n",
    "reporting = ReportingAgent()\n",
    "\n",
    "print(\"Agents initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "First, let's collect data from Databricks system tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data ingestion\n",
    "async def test_ingestion():\n",
    "    # Connect to Databricks\n",
    "    connected = await orchestrator.data_agent.connect()\n",
    "    if not connected:\n",
    "        print(\"Failed to connect to Databricks\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Connected to Databricks successfully\")\n",
    "    \n",
    "    # Collect sample data (last 2 hours)\n",
    "    data = await orchestrator.data_agent.collect_all_data(hours_back=2)\n",
    "    \n",
    "    print(f\"Collected data:\")\n",
    "    for key, df in data.items():\n",
    "        print(f\"  {key}: {len(df)} records\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Run data ingestion\n",
    "data = await test_ingestion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis\n",
    "\n",
    "Analyze network patterns and connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test network analysis\n",
    "async def test_network_analysis():\n",
    "    if data is None or 'audit_logs' not in data:\n",
    "        print(\"No audit data available\")\n",
    "        return None\n",
    "    \n",
    "    audit_df = data['audit_logs']\n",
    "    print(f\"Analyzing {len(audit_df)} audit events...\")\n",
    "    \n",
    "    # Run network analysis\n",
    "    network_results = await orchestrator.network_agent.analyze_network_activity(audit_df)\n",
    "    \n",
    "    print(\"Network Analysis Results:\")\n",
    "    print(f\"  Risk Score: {network_results.get('risk_score', 'N/A')}/10\")\n",
    "    print(f\"  Events Analyzed: {network_results.get('analyzed_events', 0)}\")\n",
    "    \n",
    "    if 'key_findings' in network_results:\n",
    "        print(\"\\nKey Findings:\")\n",
    "        for finding in network_results['key_findings'][:5]:\n",
    "            print(f\"  - {finding}\")\n",
    "    \n",
    "    return network_results\n",
    "\n",
    "# Run network analysis\n",
    "network_results = await test_network_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Security Analysis\n",
    "\n",
    "Analyze security threats and compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test security analysis\n",
    "async def test_security_analysis():\n",
    "    if data is None or 'audit_logs' not in data:\n",
    "        print(\"No audit data available\")\n",
    "        return None\n",
    "    \n",
    "    audit_df = data['audit_logs']\n",
    "    print(f\"Analyzing security in {len(audit_df)} audit events...\")\n",
    "    \n",
    "    # Run security analysis\n",
    "    security_results = await orchestrator.security_agent.analyze_security_threats(audit_df)\n",
    "    \n",
    "    print(\"Security Analysis Results:\")\n",
    "    print(f\"  Threat Level: {security_results.get('threat_level', 'UNKNOWN')}\")\n",
    "    print(f\"  Events Analyzed: {security_results.get('analyzed_events', 0)}\")\n",
    "    \n",
    "    if 'security_findings' in security_results:\n",
    "        print(\"\\nSecurity Findings:\")\n",
    "        for finding in security_results['security_findings'][:5]:\n",
    "            print(f\"  - {finding}\")\n",
    "    \n",
    "    return security_results\n",
    "\n",
    "# Run security analysis\n",
    "security_results = await test_security_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Analysis Workflow\n",
    "\n",
    "Run the complete analysis workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete analysis\n",
    "async def run_full_analysis():\n",
    "    print(\"Running full analysis workflow...\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Run analysis\n",
    "    results = await orchestrator.run_full_analysis(hours_back=2)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    print(f\"\\nAnalysis completed in {duration:.2f} seconds\")\n",
    "    print(f\"Overall Threat Level: {results.get('overall_threat_level', 'UNKNOWN')}\")\n",
    "    \n",
    "    # Display key metrics\n",
    "    data_summary = results.get('data_summary', {})\n",
    "    print(f\"\\nData Summary:\")\n",
    "    print(f\"  Audit Events: {data_summary.get('audit_events', 0)}\")\n",
    "    print(f\"  Clusters: {data_summary.get('clusters', 0)}\")\n",
    "    print(f\"  Queries: {data_summary.get('queries', 0)}\")\n",
    "    \n",
    "    # Display recommendations\n",
    "    recommendations = results.get('recommendations', [])\n",
    "    if recommendations:\n",
    "        print(f\"\\nRecommendations:\")\n",
    "        for rec in recommendations[:5]:\n",
    "            print(f\"  - {rec}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run full analysis\n",
    "full_results = await run_full_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Report\n",
    "\n",
    "Create a comprehensive report from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate report\n",
    "async def generate_report():\n",
    "    if full_results is None:\n",
    "        print(\"No analysis results available\")\n",
    "        return\n",
    "    \n",
    "    print(\"Generating analysis report...\")\n",
    "    \n",
    "    # Generate full report\n",
    "    report_result = await reporting.generate_report(full_results, report_type=\"full\")\n",
    "    \n",
    "    print(f\"Report generated: {report_result['metadata']['file_path']}\")\n",
    "    \n",
    "    # Display report summary\n",
    "    report_path = Path(report_result['metadata']['file_path'])\n",
    "    if report_path.exists():\n",
    "        with open(report_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            # Display first 1000 characters\n",
    "            print(\"\\nReport Preview:\")\n",
    "            print(\"=\" * 50)\n",
    "            print(content[:1000] + \"...\")\n",
    "            print(\"=\" * 50)\n",
    "\n",
    "# Generate report\n",
    "await generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation & Scoring\n",
    "\n",
    "Evaluate the quality of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate analysis quality\n",
    "async def evaluate_analysis():\n",
    "    if full_results is None:\n",
    "        print(\"No analysis results to evaluate\")\n",
    "        return\n",
    "    \n",
    "    print(\"Evaluating analysis quality...\")\n",
    "    \n",
    "    evaluation = await orchestrator.scoring_system.evaluate_analysis_results(full_results)\n",
    "    \n",
    "    print(f\"Overall Score: {evaluation.get('overall_score', 0):.2f}/10\")\n",
    "    \n",
    "    agent_scores = evaluation.get('agent_scores', {})\n",
    "    print(f\"\\nAgent Scores:\")\n",
    "    for agent, score in agent_scores.items():\n",
    "        print(f\"  {agent}: {score.get('performance_score', 0)}/10\")\n",
    "    \n",
    "    recommendations = evaluation.get('recommendations', [])\n",
    "    if recommendations:\n",
    "        print(f\"\\nImprovement Recommendations:\")\n",
    "        for rec in recommendations[:3]:\n",
    "            print(f\"  - {rec}\")\n",
    "\n",
    "# Evaluate analysis\n",
    "await evaluate_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Data ingestion from Databricks system tables\n",
    "2. Network pattern analysis\n",
    "3. Security threat detection\n",
    "4. Full workflow orchestration\n",
    "5. Report generation\n",
    "6. Quality evaluation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Configure production credentials\n",
    "- Set up scheduled analysis jobs\n",
    "- Customize analysis parameters\n",
    "- Integrate with alerting systems\n",
    "- Deploy to Databricks Jobs for automated monitoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}